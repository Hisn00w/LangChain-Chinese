# å‰ç«¯

> ä½¿ç”¨ LangChain æ™ºèƒ½ä½“ã€LangGraph å›¾å’Œè‡ªå®šä¹‰ API æ„å»ºå…·æœ‰å®æ—¶æµå¼ä¼ è¾“åŠŸèƒ½çš„ç”Ÿæˆå¼ UI

`useStream` React é’©å­æä¾›ä¸ LangGraph æµå¼ä¼ è¾“åŠŸèƒ½çš„æ— ç¼é›†æˆã€‚å®ƒå¤„ç†æµå¼ä¼ è¾“ã€çŠ¶æ€ç®¡ç†å’Œåˆ†æ”¯é€»è¾‘çš„æ‰€æœ‰å¤æ‚æ€§ï¼Œè®©æ‚¨ä¸“æ³¨äºæ„å»ºå‡ºè‰²çš„ç”Ÿæˆå¼ UI ä½“éªŒã€‚

ä¸»è¦åŠŸèƒ½ï¼š

* **æ¶ˆæ¯æµå¼ä¼ è¾“** â€” å¤„ç†æ¶ˆæ¯å—çš„æµä»¥å½¢æˆå®Œæ•´æ¶ˆæ¯
* **è‡ªåŠ¨çŠ¶æ€ç®¡ç†** â€” ç”¨äºæ¶ˆæ¯ã€ä¸­æ–­ã€åŠ è½½çŠ¶æ€å’Œé”™è¯¯
* **å¯¹è¯åˆ†æ”¯** â€” ä»èŠå¤©å†å²ä¸­çš„ä»»ä½•ç‚¹åˆ›å»ºå¤‡é€‰å¯¹è¯è·¯å¾„
* **UI æ— å…³è®¾è®¡** â€” ä½¿ç”¨æ‚¨è‡ªå·±çš„ç»„ä»¶å’Œæ ·å¼

## å®‰è£…

å®‰è£… LangGraph SDK ä»¥åœ¨ React åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ `useStream` é’©å­ï¼š

## åŸºæœ¬ç”¨æ³•

`useStream` é’©å­è¿æ¥åˆ°ä»»ä½• LangGraph å›¾ï¼Œæ— è®ºæ˜¯è¿è¡Œåœ¨æ‚¨è‡ªå·±çš„ç«¯ç‚¹è¿˜æ˜¯ä½¿ç”¨ [LangSmith éƒ¨ç½²](/langsmith/deployments) éƒ¨ç½²çš„ã€‚

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";

function Chat() {
  const stream = useStream({
    assistantId: "agent",
    // æœ¬åœ°å¼€å‘
    apiUrl: "http://localhost:2024",
    // ç”Ÿäº§éƒ¨ç½²ï¼ˆLangSmith æ‰˜ç®¡ï¼‰
    // apiUrl: "https://your-deployment.us.langgraph.app"
  });

  const handleSubmit = (message: string) => {
    stream.submit({
      messages: [
        { content: message, type: "human" }
      ],
    });
  };

  return (
    <div>
      {stream.messages.map((message, idx) => (
        <div key={message.id ?? idx}>
          {message.type}: {message.content}
        </div>
      ))}

      {stream.isLoading && <div>åŠ è½½ä¸­...</div>}
      {stream.error && <div>é”™è¯¯: {stream.error.message}</div>}
    </div>
  );
}
```

äº†è§£å¦‚ä½•[å°†æ‚¨çš„æ™ºèƒ½ä½“éƒ¨ç½²åˆ° LangSmith](/oss/python/langchain/deploy)ï¼Œä»¥è·å¾—å…·æœ‰å†…ç½®å¯è§‚æµ‹æ€§ã€èº«ä»½éªŒè¯å’Œæ‰©å±•çš„ç”Ÿäº§çº§æ‰˜ç®¡ã€‚

**`useStream` å‚æ•°ï¼š**

| å‚æ•° | ç±»å‹ | æè¿° |
|------|------|------|
| `assistantId` | stringï¼ˆå¿…éœ€ï¼‰ | è¦è¿æ¥çš„æ™ºèƒ½ä½“çš„ IDã€‚ä½¿ç”¨ LangSmith éƒ¨ç½²æ—¶ï¼Œè¿™å¿…é¡»ä¸éƒ¨ç½²ä»ªè¡¨æ¿ä¸­æ˜¾ç¤ºçš„æ™ºèƒ½ä½“ ID åŒ¹é…ã€‚å¯¹äºè‡ªå®šä¹‰ API éƒ¨ç½²æˆ–æœ¬åœ°å¼€å‘ï¼Œè¿™å¯ä»¥æ˜¯æœåŠ¡å™¨ç”¨æ¥è¯†åˆ«æ™ºèƒ½ä½“çš„ä»»ä½•å­—ç¬¦ä¸²ã€‚ |
| `apiUrl` | string | LangGraph æœåŠ¡å™¨çš„ URLã€‚æœ¬åœ°å¼€å‘é»˜è®¤ä¸º `http://localhost:2024`ã€‚ |
| `apiKey` | string | èº«ä»½éªŒè¯çš„ API å¯†é’¥ã€‚è¿æ¥åˆ° LangSmith ä¸Šçš„éƒ¨ç½²æ™ºèƒ½ä½“æ—¶éœ€è¦ã€‚ |
| `threadId` | string | è¿æ¥åˆ°ç°æœ‰çº¿ç¨‹è€Œä¸æ˜¯åˆ›å»ºæ–°çº¿ç¨‹ã€‚å¯ç”¨äºæ¢å¤å¯¹è¯ã€‚ |
| `onThreadId` | (id: string) => void | åˆ›å»ºæ–°çº¿ç¨‹æ—¶è°ƒç”¨çš„å›è°ƒã€‚ä½¿ç”¨æ­¤æ–¹æ³•æŒä¹…åŒ–çº¿ç¨‹ ID ä»¥ä¾›ä»¥åä½¿ç”¨ã€‚ |
| `reconnectOnMount` | boolean \| (() => Storage) | ç»„ä»¶æŒ‚è½½æ—¶è‡ªåŠ¨æ¢å¤æ­£åœ¨è¿›è¡Œçš„è¿è¡Œã€‚è®¾ç½®ä¸º `true` ä»¥ä½¿ç”¨ä¼šè¯å­˜å‚¨ï¼Œæˆ–æä¾›è‡ªå®šä¹‰å­˜å‚¨å‡½æ•°ã€‚ |
| `onCreated` | (run: Run) => void | åˆ›å»ºæ–°è¿è¡Œæ—¶è°ƒç”¨çš„å›è°ƒã€‚å¯ç”¨äºæŒä¹…åŒ–è¿è¡Œå…ƒæ•°æ®ä»¥è¿›è¡Œæ¢å¤ã€‚ |
| `onError` | (error: Error) => void | æµå¼ä¼ è¾“è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶è°ƒç”¨ã€‚ |
| `onFinish` | (state: StateType, run?: Run) => void | æµæˆåŠŸå®Œæˆæ—¶è°ƒç”¨ï¼ŒåŒ…å«æœ€ç»ˆçŠ¶æ€ã€‚ |
| `onCustomEvent` | (data: unknown, context: { mutate }) => void | å¤„ç†ä½¿ç”¨ `writer` ä»æ™ºèƒ½ä½“å‘å‡ºçš„è‡ªå®šä¹‰äº‹ä»¶ã€‚è¯·å‚é˜…[è‡ªå®šä¹‰æµå¼ä¼ è¾“äº‹ä»¶](#custom-streaming-events)ã€‚ |
| `onUpdateEvent` | (data: unknown, context: { mutate }) => void | åœ¨æ¯ä¸ªå›¾æ­¥éª¤ä¹‹åå¤„ç†çŠ¶æ€æ›´æ–°äº‹ä»¶ã€‚ |
| `onMetadataEvent` | (metadata: { run_id, thread_id }) => void | å¤„ç†åŒ…å«è¿è¡Œå’Œçº¿ç¨‹ä¿¡æ¯çš„å…ƒæ•°æ®äº‹ä»¶ã€‚ |
| `messagesKey` | string | å›¾çŠ¶æ€ä¸­åŒ…å«æ¶ˆæ¯æ•°ç»„çš„é”®ã€‚é»˜è®¤ä¸º `messages`ã€‚ |
| `throttle` | boolean | æ‰¹é‡çŠ¶æ€æ›´æ–°ä»¥è·å¾—æ›´å¥½çš„æ¸²æŸ“æ€§èƒ½ã€‚ç¦ç”¨ä»¥è·å¾—å³æ—¶æ›´æ–°ã€‚é»˜è®¤ä¸º `true`ã€‚ |
| `initialValues` | StateType \| null | åœ¨ç¬¬ä¸€ä¸ªæµåŠ è½½æ—¶æ˜¾ç¤ºçš„åˆå§‹çŠ¶æ€å€¼ã€‚å¯ç”¨äºç«‹å³æ˜¾ç¤ºç¼“å­˜çš„çº¿ç¨‹æ•°æ®ã€‚ |

**`useStream` è¿”å›å€¼ï¼š**

| å±æ€§ | ç±»å‹ | æè¿° |
|------|------|------|
| `messages` | Message[] | å½“å‰çº¿ç¨‹ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼ŒåŒ…æ‹¬äººç±»å’Œ AI æ¶ˆæ¯ã€‚ |
| `values` | StateType | å½“å‰å›¾çŠ¶æ€å€¼ã€‚ç±»å‹ä»æ™ºèƒ½ä½“æˆ–å›¾ç±»å‹å‚æ•°æ¨æ–­ã€‚ |
| `isLoading` | boolean | å½“å‰æ˜¯å¦æœ‰æµæ­£åœ¨è¿›è¡Œã€‚ä½¿ç”¨æ­¤é€‰é¡¹æ˜¾ç¤ºåŠ è½½æŒ‡ç¤ºå™¨ã€‚ |
| `error` | Error \| null | æµå¼ä¼ è¾“è¿‡ç¨‹ä¸­å‘ç”Ÿçš„ä»»ä½•é”™è¯¯ã€‚æ— é”™è¯¯æ—¶ä¸º `null`ã€‚ |
| `interrupt` | Interrupt \| undefined | å½“å‰éœ€è¦ç”¨æˆ·è¾“å…¥çš„ä¸­æ–­ï¼Œä¾‹å¦‚äººå·¥ä»‹å…¥æ‰¹å‡†è¯·æ±‚ã€‚ |
| `toolCalls` | ToolCallWithResult[] | æ‰€æœ‰æ¶ˆæ¯ä¸­çš„æ‰€æœ‰å·¥å…·è°ƒç”¨ï¼ŒåŒ…æ‹¬å…¶ç»“æœå’ŒçŠ¶æ€ï¼ˆ`pending`ã€`completed` æˆ– `error`ï¼‰ã€‚ |
| `submit` | (input, options?) => Promise<void> | å‘æ™ºèƒ½ä½“æäº¤æ–°è¾“å…¥ã€‚å½“ä½¿ç”¨å‘½ä»¤ä»ä¸­æ–­æ¢å¤æ—¶ä¼ é€’ `null` ä½œä¸ºè¾“å…¥ã€‚é€‰é¡¹åŒ…æ‹¬ç”¨äºåˆ†æ”¯çš„ `checkpoint`ã€ç”¨äºä¹è§‚æ›´æ–°çš„ `optimisticValues` å’Œç”¨äºä¹è§‚çº¿ç¨‹åˆ›å»ºçš„ `threadId`ã€‚ |
| `stop` | () => void | ç«‹å³åœæ­¢å½“å‰æµã€‚ |
| `joinStream` | (runId: string) => void | é€šè¿‡è¿è¡Œ ID æ¢å¤ç°æœ‰æµã€‚ä¸ `onCreated` ä¸€èµ·ç”¨äºæ‰‹åŠ¨æµæ¢å¤ã€‚ |
| `setBranch` | (branch: string) => void | åˆ‡æ¢åˆ°å¯¹è¯å†å²ä¸­çš„ä¸åŒåˆ†æ”¯ã€‚ |
| `getToolCalls` | (message) => ToolCall[] | è·å–ç‰¹å®š AI æ¶ˆæ¯çš„æ‰€æœ‰å·¥å…·è°ƒç”¨ã€‚ |
| `getMessagesMetadata` | (message) => MessageMetadata | è·å–æ¶ˆæ¯çš„å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬æµå¼ä¼ è¾“ä¿¡æ¯ï¼ˆå¦‚ç”¨äºæ ‡è¯†æºèŠ‚ç‚¹çš„ `langgraph_node` å’Œç”¨äºåˆ†æ”¯çš„ `firstSeenState`ï¼‰ã€‚ |
| `experimental_branchTree` | BranchTree | çº¿ç¨‹çš„æ ‘è¡¨ç¤ºï¼Œç”¨äºéåŸºäºæ¶ˆæ¯çš„å›¾ä¸­çš„é«˜çº§åˆ†æ”¯æ§åˆ¶ã€‚ |

## çº¿ç¨‹ç®¡ç†

ä½¿ç”¨å†…ç½®çº¿ç¨‹ç®¡ç†è·Ÿè¸ªå¯¹è¯ã€‚æ‚¨å¯ä»¥è®¿é—®å½“å‰çº¿ç¨‹ IDï¼Œå¹¶åœ¨åˆ›å»ºæ–°çº¿ç¨‹æ—¶æ”¶åˆ°é€šçŸ¥ï¼š

```tsx
import { useState } from "react";
import { useStream } from "@langchain/langgraph-sdk/react";

function Chat() {
  const [threadId, setThreadId] = useState<string | null>(null);

  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
    threadId: threadId,
    onThreadId: setThreadId,
  });

  // åˆ›å»ºæ–°çº¿ç¨‹æ—¶æ›´æ–° threadId
  // å°†å…¶å­˜å‚¨åœ¨ URL å‚æ•°æˆ– localStorage ä¸­ä»¥å®ç°æŒä¹…åŒ–
}
```

æˆ‘ä»¬å»ºè®®å­˜å‚¨ `threadId` ä»¥è®©ç”¨æˆ·åœ¨é¡µé¢åˆ·æ–°åæ¢å¤å¯¹è¯ã€‚

### é¡µé¢åˆ·æ–°åæ¢å¤

é€šè¿‡è®¾ç½® `reconnectOnMount: true`ï¼Œ`useStream` é’©å­å¯ä»¥åœ¨æŒ‚è½½æ—¶è‡ªåŠ¨æ¢å¤æ­£åœ¨è¿›è¡Œçš„è¿è¡Œã€‚è¿™å¯¹äºåœ¨é¡µé¢åˆ·æ–°åç»§ç»­æµå¾ˆæœ‰ç”¨ï¼Œç¡®ä¿ä¸ä¼šä¸¢å¤±åœæœºæœŸé—´ç”Ÿæˆçš„æ¶ˆæ¯å’Œäº‹ä»¶ã€‚

```tsx
const stream = useStream({
  apiUrl: "http://localhost:2024",
  assistantId: "agent",
  reconnectOnMount: true,
});
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ›å»ºçš„è¿è¡Œçš„ ID å­˜å‚¨åœ¨ `window.sessionStorage` ä¸­ï¼Œå¯ä»¥é€šè¿‡ä¼ é€’è‡ªå®šä¹‰å­˜å‚¨å‡½æ•°æ¥äº¤æ¢ï¼š

```tsx
const stream = useStream({
  apiUrl: "http://localhost:2024",
  assistantId: "agent",
  reconnectOnMount: () => window.localStorage,
});
```

è¦æ‰‹åŠ¨æ§åˆ¶æ¢å¤è¿‡ç¨‹ï¼Œä½¿ç”¨è¿è¡Œå›è°ƒæŒä¹…åŒ–å…ƒæ•°æ®å¹¶ä½¿ç”¨ `joinStream` æ¥æ¢å¤ï¼š

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";
import { useEffect, useRef } from "react";

function Chat({ threadId }: { threadId: string | null }) {
  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
    threadId,
    onCreated: (run) => {
      // æµå¼€å§‹æ—¶æŒä¹…åŒ–è¿è¡Œ ID
      window.sessionStorage.setItem(`resume:${run.thread_id}`, run.run_id);
    },
    onFinish: (_, run) => {
      // æµå®Œæˆæ—¶æ¸…ç†
      window.sessionStorage.removeItem(`resume:${run?.thread_id}`);
    },
  });

  // æŒ‚è½½æ—¶æ¢å¤æµï¼ˆå¦‚æœæœ‰å­˜å‚¨çš„è¿è¡Œ IDï¼‰
  const joinedThreadId = useRef<string | null>(null);
  useEffect(() => {
    if (!threadId) return;
    const runId = window.sessionStorage.getItem(`resume:${threadId}`);
    if (runId && joinedThreadId.current !== threadId) {
      stream.joinStream(runId);
      joinedThreadId.current = threadId;
    }
  }, [threadId]);

  const handleSubmit = (text: string) => {
    // ä½¿ç”¨ streamResumable ç¡®ä¿äº‹ä»¶ä¸ä¼šä¸¢å¤±
    stream.submit(
      { messages: [{ type: "human", content: text }] },
      { streamResumable: true }
    );
  };
}
```

æŸ¥çœ‹ `session-persistence` ç¤ºä¾‹ä¸­å¸¦æœ‰ `reconnectOnMount` å’Œçº¿ç¨‹æŒä¹…åŒ–çš„æµæ¢å¤çš„å®Œæ•´å®ç°ã€‚

## ä¹è§‚æ›´æ–°

æ‚¨å¯ä»¥åœ¨æ‰§è¡Œç½‘ç»œè¯·æ±‚ä¹‹å‰ä¹è§‚åœ°æ›´æ–°å®¢æˆ·ç«¯çŠ¶æ€ï¼Œå‘ç”¨æˆ·æä¾›å³æ—¶åé¦ˆï¼š

```tsx
const stream = useStream({
  apiUrl: "http://localhost:2024",
  assistantId: "agent",
});

const handleSubmit = (text: string) => {
  const newMessage = { type: "human" as const, content: text };

  stream.submit(
    { messages: [newMessage] },
    {
      optimisticValues(prev) {
        const prevMessages = prev.messages ?? [];
        return { ...prev, messages: [...prevMessages, newMessage] };
      },
    }
  );
};
```

### ä¹è§‚çº¿ç¨‹åˆ›å»º

åœ¨ `submit` ä¸­ä½¿ç”¨ `threadId` é€‰é¡¹æ¥å¯ç”¨ä¹è§‚ UI æ¨¡å¼ï¼Œå½“æ‚¨éœ€è¦åœ¨çº¿ç¨‹åˆ›å»ºä¹‹å‰çŸ¥é“çº¿ç¨‹ ID æ—¶ï¼š

```tsx
import { useState } from "react";
import { useStream } from "@langchain/langgraph-sdk/react";

function Chat() {
  const [threadId, setThreadId] = useState<string | null>(null);
  const [optimisticThreadId] = useState(() => crypto.randomUUID());

  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
    threadId,
    onThreadId: setThreadId,
  });

  const handleSubmit = (text: string) => {
    // ç«‹å³å¯¼èˆªè€Œä¸ç­‰å¾…çº¿ç¨‹åˆ›å»º
    window.history.pushState({}, "", `/threads/${optimisticThreadId}`);

    // ä½¿ç”¨é¢„å®š ID åˆ›å»ºçº¿ç¨‹
    stream.submit(
      { messages: [{ type: "human", content: text }] },
      { threadId: optimisticThreadId }
    );
  };
}
```

### ç¼“å­˜çº¿ç¨‹æ˜¾ç¤º

ä½¿ç”¨ `initialValues` é€‰é¡¹åœ¨ä»æœåŠ¡å™¨åŠ è½½å†å²è®°å½•æ—¶ç«‹å³æ˜¾ç¤ºç¼“å­˜çš„çº¿ç¨‹æ•°æ®ï¼š

```tsx
function Chat({ threadId, cachedData }) {
  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
    threadId,
    initialValues: cachedData?.values,
  });

  // ç«‹å³æ˜¾ç¤ºç¼“å­˜çš„æ¶ˆæ¯ï¼Œç„¶ååœ¨æœåŠ¡å™¨å“åº”æ—¶æ›´æ–°
}
```

## åˆ†æ”¯

é€šè¿‡ç¼–è¾‘ä¹‹å‰çš„æ¶ˆæ¯æˆ–é‡æ–°ç”Ÿæˆ AI å“åº”æ¥åˆ›å»ºå¤‡é€‰å¯¹è¯è·¯å¾„ã€‚ä½¿ç”¨ `getMessagesMetadata()` è®¿é—®åˆ†æ”¯çš„æ£€æŸ¥ç‚¹ä¿¡æ¯ï¼š

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";
import { BranchSwitcher } from "./BranchSwitcher";

function Chat() {
  const stream = useStream({
    apiUrl: "http://localhost:2024",
    assistantId: "agent",
  });

  return (
    <div>
      {stream.messages.map((message) => {
        const meta = stream.getMessagesMetadata(message);
        const parentCheckpoint = meta?.firstSeenState?.parent_checkpoint;

        return (
          <div key={message.id}>
            <div>{message.content as string}</div>

            {/* ç¼–è¾‘äººç±»æ¶ˆæ¯ */}
            {message.type === "human" && (
              <button
                onClick={() => {
                  const newContent = prompt("ç¼–è¾‘æ¶ˆæ¯:", message.content as string);
                  if (newContent) {
                    stream.submit(
                      { messages: [{ type: "human", content: newContent }] },
                      { checkpoint: parentCheckpoint }
                    );
                  }
                }}
              >
                ç¼–è¾‘
              </button>
            )}

            {/* é‡æ–°ç”Ÿæˆ AI æ¶ˆæ¯ */}
            {message.type === "ai" && (
              <button
                onClick={() => stream.submit(undefined, { checkpoint: parentCheckpoint })}
              >
                é‡æ–°ç”Ÿæˆ
              </button>
            )}

            {/* åœ¨åˆ†æ”¯ä¹‹é—´åˆ‡æ¢ */}
            <BranchSwitcher
              branch={meta?.branch}
              branchOptions={meta?.branchOptions}
              onSelect={(branch) => stream.setBranch(branch)}
            />
          </div>
        );
      })}
    </div>
  );
}

/**
 * ç”¨äºåœ¨å¯¹è¯åˆ†æ”¯ä¹‹é—´å¯¼èˆªçš„ç»„ä»¶ã€‚
 * æ˜¾ç¤ºå½“å‰åˆ†æ”¯ä½ç½®å¹¶å…è®¸åœ¨å¤‡é€‰æ–¹æ¡ˆä¹‹é—´åˆ‡æ¢ã€‚
 */
export function BranchSwitcher({
  branch,
  branchOptions,
  onSelect,
}: {
  branch: string | undefined;
  branchOptions: string[] | undefined;
  onSelect: (branch: string) => void;
}) {
  if (!branchOptions || !branch) return null;
  const index = branchOptions.indexOf(branch);

  return (
    <div className="flex items-center gap-2">
      <button
        type="button"
        disabled={index <= 0}
        onClick={() => onSelect(branchOptions[index - 1])}
      >
        â†
      </button>
      <span>{index + 1} / {branchOptions.length}</span>
      <button
        type="button"
        disabled={index >= branchOptions.length - 1}
        onClick={() => onSelect(branchOptions[index + 1])}
      >
        â†’
      </button>
    </div>
  );
}
```

æŸ¥çœ‹ `branching-chat` ç¤ºä¾‹ä¸­å¸¦æœ‰ç¼–è¾‘ã€é‡æ–°ç”Ÿæˆå’Œåˆ†æ”¯åˆ‡æ¢çš„å¯¹è¯åˆ†æ”¯çš„å®Œæ•´å®ç°ã€‚

## ç±»å‹å®‰å…¨çš„æµå¼ä¼ è¾“

å½“ä¸é€šè¿‡ @[`createAgent`] åˆ›å»ºçš„æ™ºèƒ½ä½“æˆ–ä½¿ç”¨ [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) åˆ›å»ºçš„å›¾ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œ`useStream` é’©å­æ”¯æŒå®Œæ•´ç±»å‹æ¨æ–­ã€‚ä¼ é€’ `typeof agent` æˆ– `typeof graph` ä½œä¸ºç±»å‹å‚æ•°ä»¥è‡ªåŠ¨æ¨æ–­å·¥å…·è°ƒç”¨ç±»å‹ã€‚

### ä½¿ç”¨ `createAgent`

ä½¿ç”¨ @[`createAgent`] æ—¶ï¼Œå·¥å…·è°ƒç”¨ç±»å‹ä¼šè‡ªåŠ¨ä»æ‚¨æ³¨å†Œåˆ°æ™ºèƒ½ä½“çš„å·¥å…·ä¸­æ¨æ–­ï¼š

```python
from langchain import create_agent, tool

@tool
def get_weather(location: str) -> str:
    """è·å–åœ°ç‚¹çš„å¤©æ°”ã€‚"""
    return f"åœ°ç‚¹å¤©æ°”: æ™´å¤©, 22Â°C"

agent = create_agent(
    model="openai:gpt-4o-mini",
    tools=[get_weather],
)
```

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState } from "./types";

function Chat() {
  // ä½¿ç”¨æ‰‹åŠ¨å®šä¹‰çš„çŠ¶æ€ç±»å‹
  const stream = useStream<AgentState>({
    assistantId: "agent",
    apiUrl: "http://localhost:2024",
  });

  // stream.toolCalls[0].call.name é”®å…¥ä¸º "get_weather"
  // stream.toolCalls[0].call.args é”®å…¥ä¸º { location: string }
}
```

```typescript
import type { Message } from "@langchain/langgraph-sdk";

// å®šä¹‰å·¥å…·è°ƒç”¨ç±»å‹ä»¥åŒ¹é…æ‚¨çš„ Python æ™ºèƒ½ä½“
export type GetWeatherToolCall = {
  name: "get_weather";
  args: { location: string };
  id?: string;
};

export type AgentToolCalls = GetWeatherToolCall;

export interface AgentState {
  messages: Message<AgentToolCalls>[];
}
```

### ä½¿ç”¨ `StateGraph`

å¯¹äºè‡ªå®šä¹‰ [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) åº”ç”¨ç¨‹åºï¼ŒçŠ¶æ€ç±»å‹ä»å›¾çš„æ ‡æ³¨ä¸­æ¨æ–­ï¼š

```python
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from typing import TypedDict, Annotated

class State(TypedDict):
    messages: Annotated[list, add_messages]

model = ChatOpenAI(model="gpt-4o-mini")

async def agent(state: State) -> dict:
    response = await model.ainvoke(state["messages"])
    return {"messages": [response]}

workflow = StateGraph(State)
workflow.add_node("agent", agent)
workflow.add_edge(START, "agent")
workflow.add_edge("agent", END)

graph = workflow.compile()
```

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";
import type { GraphState } from "./types";

function Chat() {
  // ä½¿ç”¨æ‰‹åŠ¨å®šä¹‰çš„çŠ¶æ€ç±»å‹
  const stream = useStream<GraphState>({
    assistantId: "my-graph",
    apiUrl: "http://localhost:2024",
  });

  // stream.values æ ¹æ®æ‚¨å®šä¹‰çš„çŠ¶æ€ç±»å‹åŒ–
}
```

```typescript
import type { Message } from "@langchain/langgraph-sdk";

// å®šä¹‰çŠ¶æ€ä»¥åŒ¹é…æ‚¨ Python å›¾çš„ State TypedDict
export interface GraphState {
  messages: Message[];
}
```

### ä½¿ç”¨æ ‡æ³¨ç±»å‹

å¦‚æœæ‚¨ä½¿ç”¨ LangGraph.jsï¼Œå¯ä»¥é‡ç”¨å›¾çš„æ ‡æ³¨ç±»å‹ã€‚ç¡®ä¿åªå¯¼å…¥ç±»å‹ä»¥é¿å…å¯¼å…¥æ•´ä¸ª LangGraph.js è¿è¡Œæ—¶ï¼š

### é«˜çº§ç±»å‹é…ç½®

æ‚¨å¯ä»¥ä¸ºä¸­æ–­ã€è‡ªå®šä¹‰äº‹ä»¶å’Œå¯é…ç½®é€‰é¡¹æŒ‡å®šå…¶ä»–ç±»å‹å‚æ•°ï¼š

## æ¸²æŸ“å·¥å…·è°ƒç”¨

ä½¿ç”¨ `getToolCalls` ä» AI æ¶ˆæ¯ä¸­æå–å’Œæ¸²æŸ“å·¥å…·è°ƒç”¨ã€‚å·¥å…·è°ƒç”¨åŒ…æ‹¬è°ƒç”¨è¯¦æƒ…ã€ç»“æœï¼ˆå¦‚æœå®Œæˆï¼‰å’ŒçŠ¶æ€ã€‚

```python
from langchain import create_agent, tool

@tool
def get_weather(location: str) -> str:
    """è·å–å½“å‰ä½ç½®çš„å¤©æ°”ã€‚"""
    return f'{{"status": "success", "content": "{location}å¤©æ°”: æ™´å¤©, 22Â°C"}}'

agent = create_agent(
    model="openai:gpt-4o-mini",
    tools=[get_weather],
)
```

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState, AgentToolCalls } from "./types";
import { ToolCallCard } from "./ToolCallCard";
import { MessageBubble } from "./MessageBubble";

function Chat() {
  const stream = useStream<AgentState>({
    assistantId: "agent",
    apiUrl: "http://localhost:2024",
  });

  return (
    <div className="flex flex-col gap-4">
      {stream.messages.map((message, idx) => {
        if (message.type === "ai") {
          const toolCalls = stream.getToolCalls(message);

          if (toolCalls.length > 0) {
            return (
              <div key={message.id ?? idx} className="flex flex-col gap-2">
                {toolCalls.map((toolCall) => (
                  <ToolCallCard key={toolCall.id} toolCall={toolCall} />
                ))}
              </div>
            );
          }
        }

        return <MessageBubble key={message.id ?? idx} message={message} />;
      })}
    </div>
  );
}

export function ToolCallCard({
  toolCall,
}: {
  toolCall: ToolCallWithResult<AgentToolCalls>;
}) {
  const { call, result, state } = toolCall;

  if (call.name === "get_weather") {
    return <WeatherCard call={call} result={result} state={state} />;
  }

  return <GenericToolCallCard call={call} result={result} state={state} />;
}

export function WeatherCard({
  call,
  result,
  state,
}: {
  call: GetWeatherToolCall;
  result?: ToolMessage;
  state: ToolCallState;
}) {
  const isLoading = state === "pending";
  const parsedResult = parseToolResult(result);

  return (
    <div className="relative overflow-hidden rounded-xl">
      <div className="absolute inset-0 bg-gradient-to-br from-sky-600 to-indigo-600" />
      <div className="relative p-4">
        <div className="flex items-center gap-2 text-white/80 text-xs mb-3">
          <span className="font-medium">{call.args.location}</span>
          {isLoading && <span className="ml-auto">åŠ è½½ä¸­...</span>}
        </div>
        {parsedResult.status === "error" ? (
          <div className="bg-red-500/20 rounded-lg p-3 text-red-200 text-sm">
            {parsedResult.content}
          </div>
        ) : (
          <div className="text-white text-lg font-medium">
            {parsedResult.content || "æ­£åœ¨è·å–å¤©æ°”..."}
          </div>
        )}
      </div>
    </div>
  );
}
```

```typescript
import type { Message } from "@langchain/langgraph-sdk";

// å®šä¹‰å·¥å…·è°ƒç”¨ç±»å‹ä»¥åŒ¹é…æ‚¨çš„ Python æ™ºèƒ½ä½“çš„å·¥å…·
export type GetWeatherToolCall = {
  name: "get_weather";
  args: { location: string };
  id?: string;
};

// æ™ºèƒ½ä½“ä¸­æ‰€æœ‰å·¥å…·è°ƒç”¨çš„è”åˆç±»å‹
export type AgentToolCalls = GetWeatherToolCall;

// ä½¿ç”¨æ‚¨çš„å·¥å…·è°ƒç”¨å®šä¹‰çŠ¶æ€ç±»å‹
export interface AgentState {
  messages: Message<AgentToolCalls>[];
}

export function parseToolResult(result?: ToolMessage): {
  status: string;
  content: string;
} {
  if (!result) return { status: "pending", content: "" };
  try {
    return JSON.parse(result.content as string);
  } catch {
    return { status: "success", content: result.content as string };
  }
}
```

æŸ¥çœ‹ `tool-calling-agent` ç¤ºä¾‹ä¸­å¸¦æœ‰å¤©æ°”ã€è®¡ç®—å™¨å’Œç¬”è®°å·¥å…·çš„å·¥å…·è°ƒç”¨æ¸²æŸ“çš„å®Œæ•´å®ç°ã€‚

## è‡ªå®šä¹‰æµå¼ä¼ è¾“äº‹ä»¶

ä½¿ç”¨å·¥å…·æˆ–èŠ‚ç‚¹ä¸­çš„ `writer` ä»æ™ºèƒ½ä½“æµå¼ä¼ è¾“è‡ªå®šä¹‰æ•°æ®ã€‚ä½¿ç”¨ `onCustomEvent` å›è°ƒåœ¨ UI ä¸­å¤„ç†è¿™äº›äº‹ä»¶ã€‚

```python
import asyncio
import time
from langchain import create_agent, tool
from langchain.types import ToolRuntime

@tool
async def analyze_data(data_source: str, *, config: ToolRuntime) -> str:
    """ä½¿ç”¨è¿›åº¦æ›´æ–°åˆ†ææ•°æ®ã€‚"""
    steps = ["è¿æ¥ä¸­...", "è·å–ä¸­...", "å¤„ç†ä¸­...", "å®Œæˆ!"]

    for i, step in enumerate(steps):
        # åœ¨æ‰§è¡ŒæœŸé—´å‘å‡ºè¿›åº¦äº‹ä»¶
        if config.writer:
            config.writer({
                "type": "progress",
                "id": f"analysis-{int(time.time() * 1000)}",
                "message": step,
                "progress": ((i + 1) / len(steps)) * 100,
            })
        await asyncio.sleep(0.5)

    return '{"result": "åˆ†æå®Œæˆ"}'

agent = create_agent(
    model="openai:gpt-4o-mini",
    tools=[analyze_data],
)
```

```tsx
import { useState, useCallback } from "react";
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState } from "./types";

interface ProgressData {
  type: "progress";
  id: string;
  message: string;
  progress: number;
}

function isProgressData(data: unknown): data is ProgressData {
  return (
    typeof data === "object" &&
    data !== null &&
    "type" in data &&
    (data as ProgressData).type === "progress"
  );
}

function CustomStreamingUI() {
  const [progressData, setProgressData] = useState<Map<string, ProgressData>>(
    new Map()
  );

  const handleCustomEvent = useCallback((data: unknown) => {
    if (isProgressData(data)) {
      setProgressData((prev) => {
        const updated = new Map(prev);
        updated.set(data.id, data);
        return updated;
      });
    }
  }, []);

  const stream = useStream<AgentState>({
    assistantId: "custom-streaming",
    apiUrl: "http://localhost:2024",
    onCustomEvent: handleCustomEvent,
  });

  return (
    <div>
      {Array.from(progressData.values()).map((data) => (
        <div key={data.id} className="bg-neutral-800 rounded-lg p-4 mb-4">
          <div className="flex justify-between mb-2">
            <span className="text-sm text-white">{data.message}</span>
            <span className="text-xs text-neutral-400">{data.progress}%</span>
          </div>
          <div className="w-full bg-neutral-700 rounded-full h-2">
            <div
              className="bg-blue-500 h-2 rounded-full transition-all"
              style={{ width: `${data.progress}%` }}
            />
          </div>
        </div>
      ))}
    </div>
  );
}
```

```typescript
import type { Message } from "@langchain/langgraph-sdk";

// å®šä¹‰å·¥å…·è°ƒç”¨ä»¥åŒ¹é…æ‚¨çš„ Python æ™ºèƒ½ä½“
export type AnalyzeDataToolCall = {
  name: "analyze_data";
  args: { data_source: string };
  id?: string;
};

export type AgentToolCalls = AnalyzeDataToolCall;

export interface AgentState {
  messages: Message<AgentToolCalls>[];
}
```

æŸ¥çœ‹ `custom-streaming` ç¤ºä¾‹ä¸­å¸¦æœ‰è¿›åº¦æ¡ã€çŠ¶æ€å¾½ç« å’Œæ–‡ä»¶æ“ä½œå¡çš„è‡ªå®šä¹‰äº‹ä»¶çš„å®Œæ•´å®ç°ã€‚

## äº‹ä»¶å¤„ç†

`useStream` é’©å­æä¾›å›è°ƒé€‰é¡¹ï¼Œä½¿æ‚¨å¯ä»¥è®¿é—®ä¸åŒç±»å‹çš„æµå¼ä¼ è¾“äº‹ä»¶ã€‚æ‚¨ä¸éœ€è¦æ˜¾å¼é…ç½®æµæ¨¡å¼â€”â€”åªéœ€ä¸ºæ‚¨è¦å¤„ç†çš„äº‹ä»¶ç±»å‹ä¼ é€’å›è°ƒï¼š

### å¯ç”¨å›è°ƒ

| å›è°ƒ | æè¿° | æµæ¨¡å¼ |
|------|------|--------|
| `onUpdateEvent` | åœ¨æ¯ä¸ªå›¾æ­¥éª¤ä¹‹åæ”¶åˆ°çŠ¶æ€æ›´æ–°æ—¶è°ƒç”¨ | `updates` |
| `onCustomEvent` | ä»å›¾ä¸­æ”¶åˆ°è‡ªå®šä¹‰äº‹ä»¶æ—¶è°ƒç”¨ | `custom` |
| `onMetadataEvent` | ä½¿ç”¨è¿è¡Œå’Œçº¿ç¨‹å…ƒæ•°æ®è°ƒç”¨ | `metadata` |
| `onError` | å‘ç”Ÿé”™è¯¯æ—¶è°ƒç”¨ | - |
| `onFinish` | æµå®Œæˆæ—¶è°ƒç”¨ | - |

## å¤šæ™ºèƒ½ä½“æµå¼ä¼ è¾“

å½“ä½¿ç”¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæˆ–å…·æœ‰å¤šä¸ªèŠ‚ç‚¹çš„å›¾æ—¶ï¼Œä½¿ç”¨æ¶ˆæ¯å…ƒæ•°æ®æ¥è¯†åˆ«æ¯ä¸ªæ¶ˆæ¯æ˜¯ç”±å“ªä¸ªèŠ‚ç‚¹ç”Ÿæˆçš„ã€‚å½“å¤šä¸ª LLM å¹¶è¡Œè¿è¡Œå¹¶ä¸”æ‚¨æƒ³ç”¨ç‹¬ç‰¹çš„è§†è§‰æ ·å¼æ˜¾ç¤ºå®ƒä»¬çš„è¾“å‡ºæ—¶ï¼Œè¿™ç‰¹åˆ«æœ‰ç”¨ã€‚

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END, Send
from langgraph.graph.state import CompiledStateGraph
from langchain.messages import BaseMessage, AIMessage
from typing import TypedDict, Annotated
import operator

# ä¸ºå¤šæ ·æ€§ä½¿ç”¨ä¸åŒçš„æ¨¡å‹å®ä¾‹
analytical_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
creative_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.9)
practical_model = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)

class State(TypedDict):
    messages: Annotated[list[BaseMessage], operator.add]
    topic: str
    analytical_research: str
    creative_research: str
    practical_research: str

def fan_out_to_researchers(state: State) -> list[Send]:
    return [
        Send("researcher_analytical", state),
        Send("researcher_creative", state),
        Send("researcher_practical", state),
    ]

def dispatcher(state: State) -> dict:
    last_message = state["messages"][-1] if state["messages"] else None
    topic = last_message.content if last_message else ""
    return {"topic": topic}

async def researcher_analytical(state: State) -> dict:
    response = await analytical_model.ainvoke([
        {"role": "system", "content": "æ‚¨æ˜¯åˆ†æç ”ç©¶ä¸“å®¶ã€‚"},
        {"role": "user", "content": f"ç ”ç©¶: {state['topic']}"},
    ])
    return {
        "analytical_research": response.content,
        "messages": [AIMessage(content=response.content, name="researcher_analytical")],
    }

# ç±»ä¼¼çš„ç ”ç©¶è€…èŠ‚ç‚¹...

workflow = StateGraph(State)
workflow.add_node("dispatcher", dispatcher)
workflow.add_node("researcher_analytical", researcher_analytical)
workflow.add_node("researcher_creative", researcher_creative)
workflow.add_node("researcher_practical", researcher_practical)
workflow.add_edge(START, "dispatcher")
workflow.add_conditional_edges("dispatcher", fan_out_to_researchers)
workflow.add_edge("researcher_analytical", END)
workflow.add_edge("researcher_creative", END)
workflow.add_edge("researcher_practical", END)

agent: CompiledStateGraph = workflow.compile()
```

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState } from "./types";
import { MessageBubble } from "./MessageBubble";

// èŠ‚ç‚¹é…ç½®ç”¨äºè§†è§‰æ˜¾ç¤º
const NODE_CONFIG: Record<string, { label: string; color: string }> = {
  researcher_analytical: { label: "åˆ†æç ”ç©¶", color: "cyan" },
  researcher_creative: { label: "åˆ›æ„ç ”ç©¶", color: "purple" },
  researcher_practical: { label: "å®ç”¨ç ”ç©¶", color: "emerald" },
};

function MultiAgentChat() {
  const stream = useStream<AgentState>({
    assistantId: "parallel-research",
    apiUrl: "http://localhost:2024",
  });

  return (
    <div className="flex flex-col gap-4">
      {stream.messages.map((message, idx) => {
        if (message.type !== "ai") {
          return <MessageBubble key={message.id ?? idx} message={message} />;
        }

        const metadata = stream.getMessagesMetadata?.(message);
        const nodeName =
          (metadata?.streamMetadata?.langgraph_node as string) ||
          (message as { name?: string }).name;

        const config = nodeName ? NODE_CONFIG[nodeName] : null;

        if (!config) {
          return <MessageBubble key={message.id ?? idx} message={message} />;
        }

        return (
          <div
            key={message.id ?? idx}
            className={`bg-${config.color}-950/30 border border-${config.color}-500/30 rounded-xl p-4`}
          >
            <div className={`text-sm font-semibold text-${config.color}-400 mb-2`}>
              {config.label}
            </div>
            <div className="text-neutral-200 whitespace-pre-wrap">
              {typeof message.content === "string" ? message.content : ""}
            </div>
          </div>
        );
      })}
    </div>
  );
}
```

```typescript
import type { Message } from "@langchain/langgraph-sdk";

// çŠ¶æ€ä¸æ‚¨ Python æ™ºèƒ½ä½“çš„ State TypedDict åŒ¹é…
export interface AgentState {
  messages: Message[];
  topic: string;
  analytical_research: string;
  creative_research: string;
  practical_research: string;
}
```

æŸ¥çœ‹ `parallel-research` ç¤ºä¾‹ä¸­å¸¦æœ‰ä¸‰ä¸ªå¹¶è¡Œç ”ç©¶è€…å’Œç‹¬ç‰¹è§†è§‰æ ·å¼çš„å¤šæ™ºèƒ½ä½“æµå¼ä¼ è¾“çš„å®Œæ•´å®ç°ã€‚

## äººå·¥ä»‹å…¥

å½“æ™ºèƒ½ä½“éœ€è¦äººå·¥æ‰¹å‡†å·¥å…·æ‰§è¡Œæ—¶å¤„ç†ä¸­æ–­ã€‚åœ¨[å¦‚ä½•å¤„ç†ä¸­æ–­](/oss/python/langgraph/interrupts#pause-using-interrupt)æŒ‡å—ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚

```python
from langchain import create_agent, tool, human_in_the_loop_middleware
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver

model = ChatOpenAI(model="gpt-4o-mini")

@tool
def send_email(to: str, subject: str, body: str) -> dict:
    """å‘é€ç”µå­é‚®ä»¶ã€‚éœ€è¦äººå·¥æ‰¹å‡†ã€‚"""
    return {
        "status": "success",
        "content": f'å‘ {to} å‘é€ä¸»é¢˜ä¸º "{subject}" çš„ç”µå­é‚®ä»¶',
    }

@tool
def delete_file(path: str) -> dict:
    """åˆ é™¤æ–‡ä»¶ã€‚éœ€è¦äººå·¥æ‰¹å‡†ã€‚"""
    return {"status": "success", "content": f'æ–‡ä»¶ "{path}" å·²åˆ é™¤'}

@tool
def read_file(path: str) -> dict:
    """è¯»å–æ–‡ä»¶å†…å®¹ã€‚æ— éœ€æ‰¹å‡†ã€‚"""
    return {"status": "success", "content": f"{path} çš„å†…å®¹..."}

agent = create_agent(
    model=model,
    tools=[send_email, delete_file, read_file],
    middleware=[
        human_in_the_loop_middleware(
            interrupt_on={
                "send_email": {
                    "allowed_decisions": ["approve", "edit", "reject"],
                    "description": "ğŸ“§ å‘é€å‰æ£€æŸ¥ç”µå­é‚®ä»¶",
                },
                "delete_file": {
                    "allowed_decisions": ["approve", "reject"],
                    "description": "ğŸ—‘ï¸ ç¡®è®¤æ–‡ä»¶åˆ é™¤",
                },
                "read_file": False,  # å®‰å…¨ - è‡ªåŠ¨æ‰¹å‡†
            }
        ),
    ],
    checkpointer=MemorySaver(),
)
```

```tsx
import { useState } from "react";
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState, HITLRequest, HITLResponse } from "./types";
import { MessageBubble } from "./MessageBubble";

function HumanInTheLoopChat() {
  const stream = useStream<AgentState, { InterruptType: HITLRequest }>({
    assistantId: "human-in-the-loop",
    apiUrl: "http://localhost:2024",
  });

  const [isProcessing, setIsProcessing] = useState(false);
  const hitlRequest = stream.interrupt?.value as HITLRequest | undefined;

  const handleApprove = async (index: number) => {
    if (!hitlRequest) return;
    setIsProcessing(true);

    try {
      const decisions: HITLResponse["decisions"] =
        hitlRequest.actionRequests.map((_, i) =>
          i === index ? { type: "approve" } : { type: "approve" }
        );

      await stream.submit(null, {
        command: { resume: { decisions } as HITLResponse },
      });
    } finally {
      setIsProcessing(false);
    }
  };

  const handleReject = async (index: number, reason: string) => {
    if (!hitlRequest) return;
    setIsProcessing(true);

    try {
      const decisions: HITLResponse["decisions"] =
        hitlRequest.actionRequests.map((_, i) =>
          i === index
            ? { type: "reject", message: reason }
            : { type: "reject", message: "ä¸å…¶ä»–æ“ä½œä¸€èµ·æ‹’ç»" }
        );

      await stream.submit(null, {
        command: { resume: { decisions } as HITLResponse },
      });
    } finally {
      setIsProcessing(false);
    }
  };

  return (
    <div>
      {stream.messages.map((message, idx) => (
        <MessageBubble key={message.id ?? idx} message={message} />
      ))}

      {hitlRequest && hitlRequest.actionRequests.length > 0 && (
        <div className="bg-amber-900/20 border border-amber-500/30 rounded-xl p-4 mt-4">
          <h3 className="text-amber-400 font-semibold mb-4">
            æ“ä½œéœ€è¦æ‰¹å‡†
          </h3>

          {hitlRequest.actionRequests.map((action, idx) => (
            <div key={idx} className="bg-neutral-900 rounded-lg p-4 mb-4 last:mb-0">
              <div className="text-sm font-mono text-white mb-2">{action.name}</div>
              <pre className="text-xs bg-black rounded p-2 mb-3 overflow-x-auto">
                {JSON.stringify(action.args, null, 2)}
              </pre>
              <div className="flex gap-2">
                <button
                  onClick={() => handleApprove(idx)}
                  disabled={isProcessing}
                  className="px-3 py-1.5 bg-green-600 hover:bg-green-500 text-white text-sm rounded-lg"
                >
                  æ‰¹å‡†
                </button>
                <button
                  onClick={() => handleReject(idx, "ç”¨æˆ·æ‹’ç»")}
                  disabled={isProcessing}
                  className="px-3 py-1.5 bg-red-600 hover:bg-red-500 text-white text-sm rounded-lg"
                >
                  æ‹’ç»
                </button>
              </div>
            </div>
          ))}
        </div>
      )}
    </div>
  );
}
```

```typescript
import type { Message } from "@langchain/langgraph-sdk";

// å·¥å…·è°ƒç”¨ç±»å‹åŒ¹é…æ‚¨çš„ Python æ™ºèƒ½ä½“
export type SendEmailToolCall = {
  name: "send_email";
  args: { to: string; subject: string; body: string };
  id?: string;
};

export type DeleteFileToolCall = {
  name: "delete_file";
  args: { path: string };
  id?: string;
};

export type ReadFileToolCall = {
  name: "read_file";
  args: { path: string };
  id?: string;
};

export type AgentToolCalls = SendEmailToolCall | DeleteFileToolCall | ReadFileToolCall;

export interface AgentState {
  messages: Message<AgentToolCalls>[];
}

// HITL ç±»å‹
export interface HITLRequest {
  actionRequests: Array<{
    name: string;
    args: Record<string, unknown>;
  }>;
}

export interface HITLResponse {
  decisions: Array<
    | { type: "approve" }
    | { type: "reject"; message: string }
    | { type: "edit"; newArgs: Record<string, unknown> }
  >;
}
```

æŸ¥çœ‹ `human-in-the-loop` ç¤ºä¾‹ä¸­å¸¦æœ‰æ‰¹å‡†ã€æ‹’ç»å’Œç¼–è¾‘æ“ä½œçš„å·¥ä½œæµæ‰¹å‡†çš„å®Œæ•´å®ç°ã€‚

## æ¨ç†æ¨¡å‹

<Warning>
  æ‰©å±•æ¨ç†/æ€è€ƒæ”¯æŒç›®å‰å¤„äºå®éªŒçŠ¶æ€ã€‚æ¨ç†ä»¤ç‰Œçš„æµå¼ä¼ è¾“æ¥å£å› æä¾›å•†è€Œå¼‚ï¼ˆOpenAI vs. Anthropicï¼‰ï¼Œå¹¶ä¸”å¯èƒ½éšç€æŠ½è±¡çš„å‘å±•è€Œå˜åŒ–ã€‚
</Warning>

å½“ä½¿ç”¨å…·æœ‰æ‰©å±•æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼ˆå¦‚ OpenAI çš„æ¨ç†æ¨¡å‹æˆ– Anthropic çš„æ‰©å±•æ€è€ƒï¼‰æ—¶ï¼Œæ€è€ƒè¿‡ç¨‹åµŒå…¥åœ¨æ¶ˆæ¯å†…å®¹ä¸­ã€‚æ‚¨éœ€è¦å•ç‹¬æå–å’Œæ˜¾ç¤ºå®ƒã€‚

```python
from langchain import create_agent
from langchain_openai import ChatOpenAI

# ä½¿ç”¨æ”¯æŒæ¨ç†çš„æ¨¡å‹
# å¯¹äº OpenAI: o1, o1-mini, o1-preview
# å¯¹äº Anthropic: claude-sonnet-4-20250514 å¯ç”¨æ‰©å±•æ€è€ƒ
model = ChatOpenAI(model="o1-mini")

agent = create_agent(
    model=model,
    tools=[],  # æ¨ç†æ¨¡å‹æœ€é€‚åˆå¤æ‚çš„æ¨ç†ä»»åŠ¡
)
```

```tsx
import { useStream } from "@langchain/langgraph-sdk/react";
import type { AgentState } from "./types";
import { getReasoningFromMessage, getTextContent } from "./utils";
import { MessageBubble } from "./MessageBubble";

function ReasoningChat() {
  const stream = useStream<AgentState>({
    assistantId: "reasoning-agent",
    apiUrl: "http://localhost:2024",
  });

  return (
    <div className="flex flex-col gap-4">
      {stream.messages.map((message, idx) => {
        if (message.type === "ai") {
          const reasoning = getReasoningFromMessage(message);
          const textContent = getTextContent(message);

          return (
            <div key={message.id ?? idx}>
              {reasoning && (
                <div className="mb-4">
                  <div className="text-xs font-medium text-amber-400/80 mb-2">
                    æ¨ç†
                  </div>
                  <div className="bg-amber-950/50 border border-amber-500/20 rounded-2xl px-4 py-3">
                    <div className="text-sm text-amber-100/90 whitespace-pre-wrap">
                      {reasoning}
                    </div>
                  </div>
                </div>
              )}

              {textContent && (
                <div className="text-neutral-100 whitespace-pre-wrap">
                  {textContent}
                </div>
              )}
            </div>
          );
        }

        return <MessageBubble key={message.id ?? idx} message={message} />;
      })}

      {stream.isLoading && (
        <div className="flex items-center gap-2 text-amber-400/70">
          <span className="text-sm">æ€è€ƒä¸­...</span>
        </div>
      )}
    </div>
  );
}
```

```typescript
import type { Message } from "@langchain/langgraph-sdk";

export interface AgentState {
  messages: Message[];
}
```

```typescript
import type { Message, AIMessage } from "@langchain/langgraph-sdk";

/**
 * ä» AI æ¶ˆæ¯ä¸­æå–æ¨ç†/æ€è€ƒå†…å®¹ã€‚
 * æ”¯æŒ OpenAI æ¨ç†å’Œ Anthropic æ‰©å±•æ€è€ƒã€‚
 */
export function getReasoningFromMessage(message: Message): string | undefined {
  type MessageWithExtras = AIMessage & {
    additional_kwargs?: {
      reasoning?: {
        summary?: Array<{ type: string; text: string }>;
      };
    };
    contentBlocks?: Array<{ type: string; thinking?: string }>;
  };

  const msg = message as MessageWithExtras;

  // åœ¨ additional_kwargs ä¸­æ£€æŸ¥ OpenAI æ¨ç†
  if (msg.additional_kwargs?.reasoning?.summary) {
    const content = msg.additional_kwargs.reasoning.summary
      .filter((item) => item.type === "summary_text")
      .map((item) => item.text)
      .join("");
    if (content.trim()) return content;
  }

  // åœ¨ contentBlocks ä¸­æ£€æŸ¥ Anthropic æ€è€ƒ
  if (msg.contentBlocks?.length) {
    const thinking = msg.contentBlocks
      .filter((b) => b.type === "thinking" && b.thinking)
      .map((b) => b.thinking)
      .join("\n");
    if (thinking) return thinking;
  }

  // åœ¨ message.content æ•°ç»„ä¸­æ£€æŸ¥æ€è€ƒ
  if (Array.isArray(msg.content)) {
    const thinking = msg.content
      .filter((b): b is { type: "thinking"; thinking: string } =>
        typeof b === "object" && b?.type === "thinking" && "thinking" in b
      )
      .map((b) => b.thinking)
      .join("\n");
    if (thinking) return thinking;
  }

  return undefined;
}

/**
 * ä»æ¶ˆæ¯ä¸­æå–æ–‡æœ¬å†…å®¹ã€‚
 */
export function getTextContent(message: Message): string {
  if (typeof message.content === "string") return message.content;
  if (Array.isArray(message.content)) {
    return message.content
      .filter((c): c is { type: "text"; text: string } => c.type === "text")
      .map((c) => c.text)
      .join("");
  }
  return "";
}
```

æŸ¥çœ‹ `reasoning-agent` ç¤ºä¾‹ä¸­å¸¦æœ‰ OpenAI å’Œ Anthropic æ¨¡å‹çš„æ¨ç†ä»¤ç‰Œæ˜¾ç¤ºçš„å®Œæ•´å®ç°ã€‚

## è‡ªå®šä¹‰çŠ¶æ€ç±»å‹

å¯¹äºè‡ªå®šä¹‰ LangGraph åº”ç”¨ç¨‹åºï¼Œåœ¨çŠ¶æ€çš„æ¶ˆæ¯å±æ€§ä¸­åµŒå…¥æ‚¨çš„å·¥å…·è°ƒç”¨ç±»å‹ã€‚

## è‡ªå®šä¹‰ä¼ è¾“

å¯¹äºè‡ªå®šä¹‰ API ç«¯ç‚¹æˆ–éæ ‡å‡†éƒ¨ç½²ï¼Œå°† `transport` é€‰é¡¹ä¸ `FetchStreamTransport` ä¸€èµ·ä½¿ç”¨ä»¥è¿æ¥åˆ°ä»»ä½•æµå¼ä¼ è¾“ APIã€‚

## ç›¸å…³å†…å®¹

* [æµå¼ä¼ è¾“æ¦‚è¿°](overview.md) â€” ä½¿ç”¨ LangChain æ™ºèƒ½ä½“çš„æœåŠ¡å™¨ç«¯æµå¼ä¼ è¾“
* [useStream API å‚è€ƒ](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) â€” å®Œæ•´ API æ–‡æ¡£
* [æ™ºèƒ½ä½“èŠå¤© UI](https://python.langchain.com/ui) â€” LangChain æ™ºèƒ½ä½“çš„é¢„æ„å»ºèŠå¤©ç•Œé¢
* [äººå·¥ä»‹å…¥](../advanced-usage/human-in-the-loop.md) â€” é…ç½®äººå·¥å®¡æŸ¥çš„ä¸­æ–­
* [å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ](../advanced-usage/multi-agent/overview.md) â€” ä½¿ç”¨å¤šä¸ª LLM æ„å»ºæ™ºèƒ½ä½“

